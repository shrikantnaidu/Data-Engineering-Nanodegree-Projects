# Data-Engineering-Nanodegree-Projects

![Udacity](https://github.com/shrikantnaidu/Deep-Learning-Nanodegree-Projects/blob/master/image/Udacity.png)

This repository contains project related to Udacity's [Data Engineering Nanodegree program](https://www.udacity.com/course/deep-learning-nanodegree--nd101). It consists of the projects completed as a part of the Nanodegree Program. In PostgreSQL you will also define Fact and Dimension tables and insert data into your new tables.

* [Data Modeling with Postgres](https://github.com/shrikantnaidu/Data-Engineering-Nanodegree-Projects/tree/main/Data%20Modeling%20with%20Postgres):  Modeling user activity data for a music streaming app called Sparkify.Creating a relational database and ETL pipeline designed to optimize queries for understanding what songs users are listening to. You’ll model your data in Apache Cassandra to allow for specific queries provided by the analytics team at Sparkify.

* [Data Modeling with Apache Cassandra](https://github.com/shrikantnaidu/Data-Engineering-Nanodegree-Projects/tree/main/Data%20Modeling%20with%20Cassandra): Creating a noSQL database and ETL pipeline designed to optimize queries for understanding what songs users are listening to and modeling the data in Apache Cassandra to allow for specific queries provided by the analytics team at Sparkify.

* [Data Warehousing with AWS](https://github.com/shrikantnaidu/Data-Engineering-Nanodegree-Projects/tree/main/Data%20Warehousing%20with%20AWS): Building an ETL pipeline that extracts data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to.
 
* [Data Lake with Spark](https://github.com/shrikantnaidu/Data-Engineering-Nanodegree-Projects/tree/main/Data%20Lake%20with%20Spark):  Implementing an ETL pipeline for a data lake. The data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in the app. You will load data from S3, process the data into analytics tables using Spark, and load them back into S3. You'll deploy this Spark process on a cluster using AWS.

* [Data Pipelines with Airflow](https://github.com/shrikantnaidu/Data-Engineering-Nanodegree-Projects/tree/main/Data%20Pipelines%20with%20Airflow): Creating and automating a set of data pipelines. You’ll configure and schedule data pipelines with Airflow and monitor and debug production pipelines.

* [Data Engineering - Capstone](https://github.com/shrikantnaidu/Data-Engineering-Nanodegree-Projects/tree/main/Data%20Engineering%20-%20Capstone): Enrichment of 
I94 Immigration Data and building an ETL Pipeline for it.
